# py-qe-explain
Query Expansion Explanation

## Dataset Link
[Link](https://drive.google.com/file/d/19qkzLYnz7NiE4KeqO9ZQ2YGtSB9QBcL1/view?usp=sharing)

## Dependencies
Apart from [pylucene-10.0.0](https://dlcdn.apache.org/lucene/pylucene/), the dependencies are:
1. [pytrec_eval](https://github.com/cvangysel/pytrec_eval)
2. tqdm (For progress bars)
```bash
pip install pytrec_eval tqdm scipy numpy
```
An additional dependency is GNU `parallel`. It should be available on standard repository of any Linux distribution.

## Some preliminary results
For queries generated by `iqg.py` the `trec_eval` results are:  
(Note that this is for vanilla ideal query generated from the whole ground truth.)
```
runid                 	all	ideal_query
num_q                 	all	249
num_ret               	all	249000
num_rel               	all	17412
num_rel_ret           	all	16213
map                   	all	0.8919
gm_map                	all	0.8817
Rprec                 	all	0.8511
bpref                 	all	0.8955
recip_rank            	all	1.0000
iprec_at_recall_0.00  	all	1.0000
iprec_at_recall_0.10  	all	0.9985
iprec_at_recall_0.20  	all	0.9957
iprec_at_recall_0.30  	all	0.9900
iprec_at_recall_0.40  	all	0.9789
iprec_at_recall_0.50  	all	0.9564
iprec_at_recall_0.60  	all	0.9293
iprec_at_recall_0.70  	all	0.8905
iprec_at_recall_0.80  	all	0.8262
iprec_at_recall_0.90  	all	0.6921
iprec_at_recall_1.00  	all	0.3817
P_5                   	all	0.9912
P_10                  	all	0.9643
P_15                  	all	0.9266
P_20                  	all	0.8855
P_30                  	all	0.8100
P_100                 	all	0.4510
P_200                 	all	0.2718
P_500                 	all	0.1236
P_1000                	all	0.0651
```

# Steps to reproduce for restricted ground truth using only relevance of documents that occur in top 1000 retrieval
### 1. Download the dataset from [here](https://drive.google.com/file/d/19qkzLYnz7NiE4KeqO9ZQ2YGtSB9QBcL1/view?usp=sharing), extract it and move it to a directory called `collections` inside the project.
```
mkdir collections
tar -xvzf trec678rb.tar.gz
mv trec678rb collections
```
### 2. Index the dataset.
```
python3 indexer_trec678rb.py
```
An index will be created `indexed/trec678rb`.

### 3. Generate a `run` file of top 1000 documents retrieved by standard BM25 retrieval.
```
python3 searcher.py
```
This will produce a `run` file `test-runs/bm25.run`.

### 4. Intersect the generated `run` file with the original `qrel` file to produce a `qrel` file containing only documents that occurred in the top 1000 standard BM25 retrieval.
```
python3 intersect_run_with_qrel.py test-runs/bm25.run qrels/trec678rb.qrel qrels/bm25_intersect_trec678rb.qrel
```
This will produce a (restricted) `qrel` file `qrels/bm25_intersect_trec678rb.qrel`.

### 5. Ideal Query: [Link](https://drive.google.com/file/d/1f2nzHYwQEDosw5UQQ4MtA-fm1mihNFLS/view?usp=sharing)
The ideal queries (`tar.gz`) can be downloaded from the above Google Drive link.  
It includes the `term_weights`, `run` and `ap` file for each ideal query. Extract using:
```
tar -xvzf ideal-queries.tar.gz
```

#### Generating ideal queries
1. Way 1 (without parallelization but clean output):
```
python3 iqg.py extracted-queries/trec678 --runid ideal_query_restrict
```
This produces two files:
  - A `run` file: `ideal-queries/trec678/runs/ideal_query_restrict.run`
  - A `term_weights` file: `ideal-queries/trec678/weights/ideal_query_restrict.term_weights`

2. Way 2 (with parallelization):
```
./parallel_ideal_query_computer <number of jobs>
```
where `<number of jobs>` can be replaced by number of parallel jobs. (Default: 12)

This produces two "split" directories:
- A split `run` directory: `ideal-queries/trec678/runs/ideal_query_restrict-split/`
- A split `term_weights` directory: `ideal-queries/trec678/weights/ideal_query_restrict-split/`

These split directories will contain `run`s and `term_weight`s for each query separately. These can then be merged into single `run` and `term_weights` files using `split_dir_merger.py`:
```
python3 split_dir_merger.py ideal-queries/trec678/runs/ideal_query_restrict-split ideal-queries/trec678/runs/ideal_query_restrict.run
```
and 
```
python3 split_dir_merger.py ideal-queries/trec678/weights/ideal_query_restrict-split ideal-queries/trec678/weights/ideal_query_restrict.term_weights
```
An `ap` file for the ideal query run can be produced by:
```
mkdir ideal-queries/trec678/aps/
trec_eval -m map -q qrels/trec678rb.qrel ideal-queries/trec678/runs/ideal_query_restrict.run > ideal-queries/trec678/ap/ideal_query_restrict.ap
```

### 6. Expanded queries: [Link](https://drive.google.com/file/d/1OcH57z-IqLs2bVgw5rKXiD5XkzhrgmFy/view?usp=sharing)
The expanded queries (`tar.gz`) can be downloaded from the above Google Drive link.  
It includes the `term_weights`, `run` and `ap` file for each expanded query. Extract using:
```
tar -xvzf expanded-queries.tar.gz
```

#### Generating `run` and `ap` files (not needed if downloading)
I have taken the expanded queries weights ([link](https://drive.google.com/file/d/1PutRi-rUFQ0a4QfJ157lfHK1VXOmf3hk/view?usp=sharing)) provided by Sourav Da and generated `run` and `ap` files for them using the script `run_ap_generator.py`.  
This can also be easily parallelized using the script `parallel_run_ap_computer`.
```
./parallel_run_ap_generator <number of jobs>
```
where `<number of jobs>` can be replaced by number of parallel jobs. (Default: 12)

### 7. Computing similarity and correlation
I have not written them, just changed the i/o filenames. These are the same files as you had given:
```
cd similarity-correlation
python3 compute-correlation-mm.py <similarity measure>
```
where `<similarity measure>` can be have the values: `'j', 'j1', 'j2', 'l1', 'l2', 'n', 'n1', 'n2'`
